---
title: "Thursday"
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("cocorresp")
library("vegan")
library("clustertend")
library("factoextra")
library("NbClust")
library("dendextend")
library("dplyr")
```

Load the data we are using

```{r}
data("dune")
data("dune.env")
dummy_management <-
  as.data.frame(model.matrix(~ Management - 1, data = dune.env))
#add these to the dataset
dune.env.original <-
  dune.env #we keep a copy of the original version
dune.env <-
  dune.env %>% select(A1, Moisture, Manure, Use) %>% cbind(., dummy_management)
dune.env$Moisture <-
  as.numeric(as.character(dune.env$Moisture)) #make numeric
dune.env$Manure <- as.numeric(as.character(dune.env$Manure))
dune.env$Use <- as.numeric(dune.env$Use)
#make column names shorter
dune.env <-
  dune.env %>% rename(BF = ManagementBF,
                      HF = ManagementHF,
                      NM = ManagementNM,
                      SF = ManagementSF)
```

### [Exercise 22: Interpreting ordination results using species traits]{style="color:cornflowerblue"}

Experienced plant ecologists may already have looked at the species in
the Dune Meadow ordination graphs and concluded that species with
similar traits occur together. This is possible if you have good
knowledge about plant species' ecological preferences, and if there are
relatively few species in your dataset. In this exercise we will use
tabulated data on species ecological preferences (the Ellenberg
indicator values) to interpret the results of the ordinations. The
indicator values are abbreviated as follows- L = Light, T = Temperature,
K = Contintentality (i.e. preference for an oceanic or inland
contintental climate), F = Moisture, R = Reaction (i.e. soil pH
preference), N = Nitrogen, S = Salinity.

#### [Questions:]{style="color:forestgreen"}

#### [22.1 Which Ellenberg values are most important for the distribution of the species?]{style="color:forestgreen"}

#### [22.2 What do the different axes represent in terms of environmental gradients?]{style="color:forestgreen"}

```{r message=FALSE, warning=FALSE}
#load Ellenberg data
dune.ell <- readRDS("Data/Ellenberg.RDS")
dune.mean.ell <- readRDS("Data/Mean_Ellenberg.RDS")
#The vegan implementation of forward selection of variables does not accept NA values.
#To get around this we will replace the two NAs with the mean value for all plots in that column.
#Do not do this in a real analysis without seriously thinking about how it affects your results!
## Code to replace missing values with the column mean, and save the results as a new dataframe
## called dune.mean.ell.impute (since we are imputing the missing values).
dune.mean.ell.impute <- data.frame(sapply(dune.mean.ell,
                                          function(x)
                                            ifelse(is.na(x),
                                                   mean(x, na.rm = TRUE),
                                                   x)))
#CCA analysis
#create global model with CCA (including all variables) and test it's significance, and if it is significant,
#we use the ordistep function with appropriate arguments to do forward selection of variables.
cca1 <-
  cca(dune ~ ., data = dune.mean.ell.impute) # full model (with all explanatory variables)
anova(cca1) #overall model is significant
# Start by making ordinations
cca0 <-
  cca (dune ~ 1, data = dune.mean.ell.impute) # empty model only with intercept
cca1 <-
  cca(dune ~ ., data = dune.mean.ell.impute, na.action = na.omit) # full model (with all explanatory variables)
cca1
plot(cca1, main = "CCA, all data")
#Stepwise approach, using "ordistep"
step2 <- ordistep(cca0, scope = formula(cca1), direction = "forward")
step2$anova # which three are most significant in the final selection?
```

#### 

------------------------------------------------------------------------

### [Exercise 23: Decomposition of variance]{style="color:cornflowerblue"}

In usual analysis of variance experiments, the variance is decomposed
into components. The same can be done in multivariate analyses. In this
exercise you will decompose the variance in the Dune meadow data set
into different variance components. You will use two groups of variance
components: Group 1 is Management and Group 2 is the soil variables (A1
and Moisture). In ordinations, the variance is expressed by the sum of
the eigenvalues.

The result gives you the fraction of the total variation that is
explained by: a) uniquely by Management (effect of soil removed), b)
uniquely by soil (effect of Management removed), c) jointly by Soil and
Management (the interaction).

The total explained variation is the sum of a), b) and c).

The total variation in the dataset is the sum of all unconstrained
eigenvalues.

#### [Questions:]{style="color:forestgreen"}

#### [23.1 Perform the analyses and interpret the results - how much of the total variation (% of All) is explained by: - uniquely by Management (effect of Soil removed), - uniquely by Soil (effect of Management removed), - jointly by Soil and Management (the interaction) - Not by Soil or Management?]{style="color:forestgreen"}

```{r, warning=FALSE, message=FALSE}

## variance partitioning
management <- dune.env[, c("BF", "HF", "NM", "SF")]
soil <- dune.env[, c("A1", "Moisture")]
# examine the explanatory variable of each class of variables.
varp <- varpart(dune, management, soil)
varp
plot (
  varp,
  digits = 2,
  Xnames = c('Management', 'Soil'),
  bg = c('red', 'blue')
)
```

------------------------------------------------------------------------

### [Exercise 24: Analysing a time series with vegetation data]{style="color:cornflowerblue"}

Quite often vegetation ecologists have data from repeated inventories in
permanent plots. The overall question to answer is if there has been a
significant and directional change in the species composition. In this
exercise we have rearranged the Dune Meadow data so that it consists of
only 10 plots, each analysed 2 times (same Dune Meadow data as in all
other exercises, just grouping and rearrangement of plots). We want to
analyse if there has been a consistent change in species composition
between the two inventories. We are thus not interested in differences
between the ten plots.

#### [Questions:]{style="color:forestgreen"}

#### [24.1 Is there a significant change in species composition over time?]{style="color:forestgreen"}

#### [24.2 Why should plots be defined as co-factors in this analysis?]{style="color:forestgreen"}

```{r, warning=FALSE, message=FALSE}
#This partials out the effect of Plot before analysing the effects of Time
#First load the time series versions of the data
SpeTS <- readRDS("Data/SpeTS.RDS")
EnvTS <- readRDS("Data/EnvTS.RDS")
time.cca <- cca(SpeTS ~ Time + Condition(Plot), data = EnvTS)
time.cca
treat <- EnvTS$Time 
colvec <-
  c("red2", "blue") #set colours to be applied to different levels of factor "Time"
plot(time.cca, type = "n", display = c("wa"))#plots the axes
with(time.cca,
     points(
       time.cca,
       display = c("wa"),
       col = colvec[treat],
       #plots the points
       pch = 21,
       xlim = c(-2, 2),
       bg = colvec[treat]
     ))
```

```{r}
#permutation test
with(EnvTS, anova(time.cca, by="term", perm=999, strata=Plot))
```

```{r, warning=FALSE, message=FALSE}
#Compare with results for time as only explanatory variable and no cofactors
time2.cca <- cca(SpeTS ~ Time, data=EnvTS)
time2.cca
#permutation test
with(EnvTS, anova(time2.cca, by="term", perm=999))
```

```{r, warning=FALSE, message=FALSE}
## plot ellipsoid hulls
treat <- EnvTS$Time
plot(time2.cca, type = "n")#plots the axes
with(time2.cca, points(
  time2.cca,
  col = colvec[treat],
  #plots the points
  pch = 21,
  bg = colvec[treat]
))
ordihull(
  time2.cca,
  groups = treat,
  draw = "polygon",
  col = "grey70",
  label = T
)#draw hulls around area of each level of factor "Time"
```

------------------------------------------------------------------------

### [Exercise 25: A multivariate Before-After-Control- Impact (BACI) study]{style="color:cornflowerblue"}

In this exercise we will continue to use the rearranged Dune Meadow data
from exercise 24. A difference is that the plots are now divided into
four groups: 1. Control plots before a treatment (i.e. not treated) 2.
Control plots after a treatment (i.e. not treated) 3. Impact plots
before treatment (i.e. not treated) 4. Impact plots after treatment
(this is where the treatment is made)

The four groups are indicated in variable Treat in the environmental
data set (EnvTS). The question you want to answer is if the treatment
caused a change in species composition that is significantly different
from the change in the control plots.

#### [Questions:]{style="color:forestgreen"}

#### [25.1 Did the treatment have an effect?]{style="color:forestgreen"}

#### [25.2 What was permuted?]{style="color:forestgreen"}

```{r, warning=FALSE, message=FALSE}
baci.cca <- cca(SpeTS ~ Treat + Condition(Plot + Time), data=EnvTS)
baci.cca
```

```{r, warning=FALSE, message=FALSE}
with(EnvTS, anova(baci.cca, by="term", model = "full", permutations=999, strata=Treat))
#Here with() is a special function that makes variables in dune.env visible to
#the following command. If you only type Moisture in an R prompt, you will get
#an error of missing variables
```

```{r, warning=FALSE, message=FALSE}
## plot results
treat <- EnvTS$Treat
ordiplot(baci.cca, type = "points")
ordihull(
  baci.cca,
  groups = treat,
  draw = "polygon",
  col = "grey70",
  label = T
)#draw hulls around area of each level of factor "Time"
```

------------------------------------------------------------------------

### [Exercise 26: Comparing ordinations]{style="color:cornflowerblue"}

These are two ways of comparing ordinations or the structure in related
multivariate datasets. They are used to compare how objects differ when
they are described by different sets of descriptors, or if there are
repeated samplings of the same objects using the same descriptors. In
this exercise you will use a third data set from the dune meadow. The
new data is insect abundance at the different plots.

#### [Questions:]{style="color:forestgreen"}

#### [26.1 How similar is the arrangement of sites in the two ordinations based on plant and insect data?]{style="color:forestgreen"}

#### [26.2 Do the three analyses (Procrustes, Co-Correspondence and Mantel) indicate similar ordinations of sites based on plant vs. insect data?]{style="color:forestgreen"}

```{r ex24, inclue=TRUE, warning=FALSE, message=FALSE}
#
# DCA on the plant species data
#
plants.DCA <- decorana(dune)
#
# DCA on the insect data (called Bugs)
#
insects <- readRDS("Data/Bugs.RDS")
insects.DCA <- decorana(insects)
#
# Have a look at the DCA based on insects
#
insects.DCA
#
# perform a Procrustes analysis on the two DCAs from different datasets
#
plants.and.insects.from.DCA <- procrustes(plants.DCA, insects.DCA)
plot(plants.and.insects.from.DCA,
     kind = 1,
     type = "text")
#
# Labels show the position of the samples in the second ordination, and arrows point to their positions in the
# target ordination Function protest tests the non-randomness (`significance') between two configurations
#
ProCsig <- protest(plants.DCA, insects.DCA)
ProCsig
#
# Co-correspondence analysis
#
CoCor <- coca(dune ~ ., data = insects, method = "symmetric")
summary(CoCor)
corAxis(CoCor)
#
# set up and perform the Mantel test
#
insects.distance <- vegdist(insects)
plants.distance <- vegdist(dune)
#
# what does mantel() do?
#
help(mantel)
#
# perform the mantel test using parametric and non-parametric correlations
mantel(insects.distance, plants.distance)
mantel(insects.distance, plants.distance, method = "spear")
```

We can also take the output from one ordination and use it in another...

```{r}

bugs.ca <- cca(insects) # Make a CA on the insect data set
ca.site.scores <-
  vegan::scores(bugs.ca, c(1, 2), display = "sites") # Extract the site scores from the CA (axis 1 and2)
dune.env.bugs <-
  cbind(dune.env, ca.site.scores) # Add the site scores to the dune.env
dune.bugs.cca <-
  cca(dune~., dune.env.bugs) # Make a CCA on the plant data with CA axis 1 and 2 as additional environmental variables
dune.bugs.cca
anova(dune.bugs.cca, by = "terms")
plot(dune.bugs.cca)
```

------------------------------------------------------------------------

### [Exercise 27 : Classification 1]{style="color:cornflowerblue"}

Start by testing the non-hierarchical K-means clustering, using Multivar
/ K-Means. Note that it is recommended that K-means need at least 100
observations (500 according to some sources) to be reliable! Let us
ignore the sample size issue for the moment, and ask for 4 clusters (for
later comparison with the four management types). Test different
hierarchical agglomeration algorithms and similarity indices. Use at
least the Euclidian and the Bray-Curtis similarity measures for the
hierarchical clustering technique. Also try Ward's method

#### [Questions:]{style="color:forestgreen"}

#### [27.1 How many clusters seem to exist in the Dune data set?]{style="color:forestgreen"}

#### [27.2 Is a 3- or 4-cluster solution a better match to the management type in the dune.env dataset?]{style="color:forestgreen"}

#### [27.3 Do the two approaches to hierarchical clustering reveal similar patterns in the Dune data set?]{style="color:forestgreen"}

#### [27.4 Interpret the Hopkins statistic for the Dune data set.]{style="color:forestgreen"}

```{r}

# See how many clusters there appear to be in the data
# As K-means is based on Euclidean disatnces, we should hellinger transform our species data
dune.h <- vegdist(dune, method = "hellinger")
kkh <- cascadeKM(dune.h,inf.gr=2,sup.gr=6)
plot(kkh)
#
# K-Means Cluster Analysis based on the previously identified number of clusters
#
dune.fit.4 <- kmeans(dune.h, 4, nstart = 50) 
```

As the final result of k-means clustering result is sensitive to the
random starting assignments, we specified nstart = 50. This means that R
will try 50 different random starting assignments and then select the
best results corresponding to the one with the lowest within cluster
variation. Note that k-means clustering involves randomness so you won't
neccessarily get exactly the same results if you repeat it. There are
also several different algorithms that can be used in k-means
clustering, and different software can use different defaults.Check out
the help file for the kmeans function if you want to know more about
these.

```{r}
#cluster number for each data point
dune.fit.4$cluster
#
#cluster sizes
dune.fit.4$size
#
#compare clusters to management category
table(dune.fit.4$cluster, dune.env.original$Management)
#

```

We should also try the other family of clustering methods, hierarchical
clustering. this gives you the relationship among the samples

```{r}
#try some approaches to hierarchical clustering
dend2 <- dune %>% # data
  dist(method = "euclidean") %>% # calculate a distance matrix, choose method
  hclust(method = "ward.D") %>% # Hierarchical clustering, choose method
  as.dendrogram # Turn the object into a dendrogram.
dend2 %>% set("labels_color") %>% set("branches_lwd") %>%  set("branches_k_color", k = 4) %>% plot(main = "Euclidean")
```

The dist() function does not include Bray-Curtis but we can make a
distance matrix using the vegdist() function from vegan. Make a distance
matrix using the Bray-Curtis method

```{r}
dune.dist <- vegdist(dune, method = "bray")
#
dend4 <- dune.dist %>% # data (Bray-Curtis)
  hclust(method = "aver") %>% # Hierarchical clustering, choose method
  as.dendrogram # Turn the object into a dendrogram.
dend4 %>% set("labels_color") %>% set("branches_lwd") %>%  set("branches_k_color", k = 4) %>% plot(main = "Average")
#
#we can colour the labels based on their grouping in the kmeans analysis to compare clusters
#
dend4 %>% set("labels_color", dune.fit.4$cluster) %>% set("branches_lwd") %>%  set("branches_k_color", k = 4) %>% plot(main = "Average")
#
#tanglegram is a function to compare dendrograms that you might want to try out as an extra
#
tanglegram(dend2, dend4)
```

------------------------------------------------------------------------

### [Exercise 28: Principal Response Curves]{style="color:cornflowerblue"}

In the following exercise, we will attempt to identify temporal
structure in a data set using Principal Response Curves (PRC).

#### [Questions:]{style="color:forestgreen"}

#### [28.1 Was there an effect of dose over time on the invertebrate community?]{style="color:forestgreen"}

#### [28.2 Which species were most affected, i.e changed most under the highest dose compared to the control ?]{style="color:forestgreen"}

```{r}
# We will use the pyrifos data set, which we have not previously seen.
#
help(pyrifos)
#
# As you will have seen from the description of the pyrifos data set, we need to do some data # # manipulation before it will be possible to perform any analysis.
#
data(pyrifos)
ditch_ <- gl(12, 1, length=132)
week_ <- gl(11, 12, labels=c(-4, -1, 0.1, 1, 2, 4, 8, 12, 15, 19, 24))
dose_ <- factor(rep(c(0.1, 0, 0, 0.9, 0, 44, 6, 0.1, 44, 0.9, 0, 6), 11)) 
#
# An appropriate analysis for exploring temporal structure...
#
pyrifos.PRC<-prc(pyrifos, treatment=dose_, time=week_)
pyrifos.PRC  
#
# plot some results
#
plot(pyrifos.PRC)
#
# Plotting the total result set is a bit messy, plot only the common species based on sum of
# abundances
#
pyrifos.SumOfAbundances<-colSums(pyrifos)
plot(pyrifos.PRC,select=pyrifos.SumOfAbundances > 500)
#
## Ditches are randomized, we have a time series, and are only
## interested in the first axis
pyrifos.PRC.ctrl <- how(
  plots = Plots(strata = ditch_, type = "free"),
  within = Within(type = "series"),
  nperm = 99
)
anova(pyrifos.PRC, permutations = pyrifos.PRC.ctrl, first = TRUE)
```

------------------------------------------------------------------------

### [Exercise 29: PLS]{style="color:cornflowerblue"}

PLS regression, like PCA, seeks to find components which maximize the
variability of predictors but differs from PCA as PLS requires the
components to have maximum correlation with the response. The predictor
variables are mapped to a smaller set of variables, and within that
smaller space we perform a regression against the outcome variable. PLS
aims to choose new mapped variables that maximally explain the outcome
variable.

#### [Questions:]{style="color:forestgreen"}

#### [29.1 Which explanatory variables are most important to explain the response variables (A1 and Moisture)?]{style="color:forestgreen"}

#### [29.2 Are there any obvious outliers in the dataset?]{style="color:forestgreen"}

#### [29.3 Does the data seem to be in need of a transformation?]{style="color:forestgreen"}

```{r PLS, echo=TRUE, message=FALSE, warning=FALSE}

library(pls)

#This function takes the same form as a lot of regression models in R:
#plsr(Response variable ~ Explanatory Variables, data = yourdata, scale = TRUE/FALSE)
#However the response variable can be a matrix of variables.

#Here we will use the dune species to predict environmental variables A1 and moisture.
#We have mostly done this in the other direction, but correlations go both ways! 
pls.response <- dplyr::select(dune.env.original, A1, Moisture)
pls.response$Moisture <-  as.numeric(pls.response$Moisture)
pls.response <- as.matrix(pls.response)
pls.exp <- as.matrix(dune)
pls.fit <- plsr(pls.response ~ pls.exp,
                na.action = na.omit,
                validation = "LOO")

#Cross validation is used to help us find the optimal number of retained dimensions.
#Then the model is rebuilt with this optimal number of dimensions.
#Find the number of dimensions with lowest cross validation error
cv <- RMSEP(pls.fit)
best.dims <- which.min(cv$val[estimate = "adjCV", ,]) - 1
best.dims #3 seems optimal

summary(pls.fit)

# Rerun the model with optimal dimensions
pls.fit2 <-
  plsr(pls.response ~ pls.exp, ncomp = best.dims, na.action = na.omit)
#Finally, we extract the useful information and format the output.
coefficients <-  coef(pls.fit2)
sum.coef <-  sum(sapply(coefficients, abs))
coefficients <-  coefficients * 100 / sum.coef
coefficients <-  sort(coefficients[, 1 , 1])
barplot(tail(coefficients, 3))
#The regression coefficients are normalized so their absolute sum is 100 and the result is sorted.

#You can run the following to see the other end of the scale for negative predictors.
barplot(head(coefficients, 3))

summary(pls.fit2)
#pls.fit2$loadings
#We can plot the scores
corrplot(pls.fit2,
         comps = 1:3,
         labels = names(coefficients))

#This gives a pairwise plot of the score values for the three first components.
#Score plots are often used to look for patterns, groups or outliers in the data.
#The scores represents the different locations
plot(pls.fit2, plottype = "scores", comps = 1:3)
#study the predicted vs. measured plot to see if the data needs to be transformed
plot(pls.fit2,
     ncomp = best.dims,
     asp = 1,
     line = TRUE)


 

```
