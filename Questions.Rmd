---
title: "MVA Course"
output:
  html_document:
    df_print: paged
  pdf_document: default
  html_notebook: default
---
```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_chunk$set(eval=FALSE)
library(vegan)
```

### <span style="color:cornflowerblue">Question 1. Introduction to R Notebooks and Markdown.</span>

This exercise is only to get used to working with R Notebook/R Markdown. The data in this first exercise are not multivariate and the statistical analysis is a simple linear regression. All following exercises will start with this kind of introduction.

### <span style="color:forestgreen"> For each exercise there will be a number of explicit questions written in green. </span>
To pass the course, you should give your answers to these questions to one of the teachers during the computer labs. 

#### <span style="color:Red">Write your answers to the questions and any other comments or notes using the Answers.Rmd file as a template, or create a new file yourself. </span>

---

#### A quick note on R Notebooks and R Markdown 
These terms are often used interchangeably to mean a combination of R code, results, and formattable text, such as this one. They allow you to present your results, the code used to generate the results and your interpretation and comments all together in one document that you can easily update, and which is easy for a reader to understand (R Notebooks are a minor update of R Markdown files that allow for easier editing as you can run just part of your code and see the results in a preview. Both notebook and markdown are .Rmd files, and are written in exactly the same way, so don't worry about the difference for this course!).

---

#### Example of a simple analysis
First we load or create the data we will be using. The box below is an example of a code chunk.
```{r}
df <- read.table(header = TRUE, sep = ",", 
               text = "x,y 
               1,2
               1.5,3.5
               2,5
               2.5,5
               3,7")

# Anything you write in a code chunk after a hash mark becomes a comment like this
# and is ignored when running the code. If you are working in an R markdown notebook
# such as this however, you can also write your longer notes in the text sections 
# outside the code chunks.
```
So I could write my notes here instead, which is probably easier to read for anything more than a few words of explanation.

Now we can do a simple linear regression. The results are stored in the object "reg".
```{r}
reg <- lm(y~x, df) 
```

Plot the result.
```{r}
plot(y~x, df)
lines(df$x, predict(reg), col = 'red') # Add a regression line to the plot
```

Get a summary of the results
```{r}
summary(reg)
```


### <span style="color:cornflowerblue">The R packages and data we will be using.</span>

The "dune" dataset we will use is included in the vegan package (an extensive collection of tools for multivariate analysis), but we will also be using some slightly modified and extended versions of it, which we will load as needed. We will also load some other packages to extend the range of analyses available to us.

<br>

***

<br>

### <span style="color:cornflowerblue">Question 2: Simple exercise for illustrating effects of different distance metrics.</span>

#### <span style="color:forestgreen">Here, we will create a simple data set consisting of 2 columns of measurement made at 3 sites we will then calculate a series of distance metrics using the vegdist() function. Questions: 1. How do the metrics differ? 2. Which metric corresponds to a "standard" distance? 3. Where would one find the formula for the Mahalanobis distance?</span>

```{r, warning=FALSE, message=FALSE}
library(vegan)
# create a triangular data set with 2 columns (x and y) and 3 rows
#
dt<-data.frame(x=c(5, 5, 10), y=c(5, 10, 10))
#plot dt
plot(dt,xlim=c(0,15),ylim=c(0,15))
# what does vegdist() do?
#
help(vegdist)
#
# create some distance matrices with using different metrics
#
(dt.bray<-vegdist(dt,method="bray"))
(dt.manhattan<-vegdist(dt,method="manhattan"))
(dt.euclidean<-vegdist(dt,method="euclidean"))
(dt.mahalanobis<-vegdist(dt,method="mahalanobis"))

```
              
<br>

***
<br>

### <span style="color:cornflowerblue">Question 3: In this	exercise we will use Principal Components Analysis on the dune meadow explanatory data. We will also explore the effects of different kinds of scaling on the ordination plot. NOTE that the scaling issues applies to all ordination techniques! </span>

#### <span style="color:forestgreen"> Compare the plots on the PCA with and without scaling (PCA plot 4) and discuss the differences. 
Is there any way to see that one is on scaled data and one is not? 
What is the difference between PCA plot 4 and PCA plot 5? 
Which one is best to use? Discuss the difference between the plots in plot PCA plot 6 and 7!
</span>

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(vegan)
library(corrplot)

data("dune")
data("dune.env")
dummy_management <- as.data.frame(model.matrix( ~ Management - 1, data=dune.env )) 
#add these to the dataset
dune.env.original <- dune.env #we keep a copy of the original version
dune.env <- dune.env %>% select(A1, Moisture, Manure, Use) %>% cbind(.,dummy_management) 
dune.env$Moisture <- as.numeric(as.character(dune.env$Moisture)) #make numeric
dune.env$Manure <- as.numeric(as.character(dune.env$Manure))
dune.env$Use <- as.numeric(dune.env$Use)
#make column names shorter
dune.env <- dune.env %>% rename(BF = ManagementBF, HF = ManagementHF,
                                NM = ManagementNM, SF = ManagementSF)

## PCA plot 1:
# Start by looking at pairwise correlations among the variables.
dune.env_cor<-cor(dune.env, method = "kendall")
corrplot(dune.env_cor, type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)

## PCA plot 2:
# Then make a PCA, with scaling of the data
env.pca <- rda(dune.env, scale = TRUE) #vegan uses the same function for PCA and RDA, just depends on if it is constrained or not.
biplot(env.pca) #plot the results using the default plot scaling which is "species"
env.pca #summarise results
summary(eigenvals(env.pca)) #see variance explained

## PCA plot 3:
# Continue with a PCA on the same data, but without scaling of the data
env.pca2 <- rda(dune.env, scale = FALSE) #vegan uses the same function for PCA and RDA, just depends on if it is constrained or not.
biplot(env.pca2)#plot the results
env.pca2 #summarise results
summary(eigenvals(env.pca2)) #see variance explained

## PCA plot 4:
# Compare the PCAs with and without scaling of data
# Default scaling of the plots, which is "species"
par(mfrow = c(1,2))
biplot(env.pca, main = "PCA with scaling of data")
biplot(env.pca2, main = "PCA without scaling of data")
par(mfrow = c(1,1))

## PCA plot 5:
# Plots from the same PCA, but with plot scaling focused on sites and on species
par(mfrow = c(1,2))
biplot(env.pca, scaling = "sites", main = "Plot scaling on sites")
biplot(env.pca, scaling = "species", main = "Plot scaling on species")
par(mfrow = c(1,1))

## PCA plot 6:
# Then, a plot from the PCA on un-scaled data, 
# but with plot scaling to mimic a PCA on scaled data (correlation = TRUE)
# Second plot is the PCA on scaled data
# Both plots with plot scaling focused on species
par(mfrow = c(1,2))
biplot(env.pca2, scaling = "species", correlation = TRUE, main = "PCA on un-scaled data,\nplot scaling on correlations")
biplot(env.pca, scaling = "species", correlation = FALSE, main = "PCA on scaled data")
par(mfrow = c(1,1))

## PCA plot 7:
# Finally a plot from the PCA on un-scaled data,
# but with plot scaling to mimic a PCA on scaled data
# Second plot is the PCA on scaled data
# Both plots with plot scaling focused on sites
par(mfrow = c(1,2))
biplot(env.pca2, scaling = "sites", correlation = TRUE, main = "PCA on un-scaled data,\nplot scaling on correlations")
biplot(env.pca, scaling = "sites", main = "PCA on scaled data")
par(mfrow = c(1,1))

```
                    

<br>

***

<br>
 

### <span style="color:cornflowerblue">Question 4: Do a CA-ordination on the Dune Meadow species dataset.What are the results telling you? </span>

#### <span style="color:forestgreen">Give a conceptual description on why objects/samples and descriptors/species to the left differ from objects and descriptors to the right, and those at the bottom from those at the top! (Plant ecologists may give a more detailed description, using their knowledge about the species in the dataset). </span>



```{r, warning=FALSE, message=FALSE}
dune.ca <- cca(dune)
plot(dune.ca)
dune.ca
summary(eigenvals(dune.ca)) #proportion variance explained
```
                    

<br>

***

<br>


### <span style="color:cornflowerblue">Question 5: Repeat exercise 4, but with DCA ordination instead. </span>

#### <span style="color:forestgreen">Look at the eigenvalues, the length of gradient, the total variation and the ordination diagram. Explain the differences between results from CA and DCA. </span>

```{r, warning=FALSE, message=FALSE}
dune.dca <- decorana(dune)
dune.dca 
#Detrended correspondence analysis (function decorana).
#Note that we do not get "variation explained" in the R implementation of DCA (and some other functions).
#Here, the developer explains why, "The total amount of variation is undefined in detrended 
#correspondence analysis and therefore proportions from total are unknown and undefined. 
#DCA is not a method for decomposition of variation, and therefore
#these proportions would not make sense either.
plot(dune.dca)
```


<br>

***

<br>

### <span style="color:cornflowerblue">Question 6: Add one new species with cover degree 9 at the site with the lowest number of species. Do a new DCA-ordination.</span>

#### <span style="color:forestgreen">What happens with the eigenvalues and ordination diagram?</span>     

```{r ex6, inclue=TRUE, warning=FALSE, message=FALSE}
# calculate the number of species present in each plot
rowSums(dune != 0)
# find the plot with the lowest number of species
which.min(rowSums(dune != 0))
# create a new data frame with a new species with cover degree 9 at site 1
NewSp <- c(9, rep(0,19))
SpeciesNewSp <- data.frame(dune, NewSp)
# run a new DCA with the extra species
DCANewSp <- decorana(SpeciesNewSp)
DCANewSp
```

```{r ex6plot, inclue=TRUE, warning=FALSE, message=FALSE}
# plot the DCA
ordiplot(DCANewSp, type="text", main="DCA with new sp.")
```


You may be surprised that you haven't got any total inertia values when printing decorana results, 
although in other software (e.g. CANOCO) these are available, together with percentage variance 
explained by particular axes. The reason for this is that DCA does not support the concept of total 
inertia values (also, it produces only four axes, i.e. four eigenvalues). See the comment in the 
code for the question 5.

<br>

***

<br>

### <span style="color:cornflowerblue">Question 7:	Effects from increased variation. Add one new sample to the data, then add a cover of 9 for the existing species Chenopodium album "Chealbu" in the new sample. Make a new DCA-ordination. <br> Next, remove the Chealbu = 9, and add instead a cover of 5 for the exisiting species Juncbufo and make a new DCA. </span>

#### <span style="color:forestgreen"> What happens with the eigenvalues and ordination after adding Chealbu and after adding Juncbufo? Why did we choose to add these two species in the new sample? </span>

```{r ex7, inclue=TRUE, warning=FALSE, message=FALSE}
# figure out the column number for Chenopodium album (abbreviated "Chenalbu")
which(colnames(dune)=="Chenalbu")
# create a new data frame with a new sample containing Chenopodium album with a cover of 9
NewSample <- c(rep(0,7), 9, rep(0,22))
SpeciesNewSample <- data.frame(rbind(dune, NewSample))
# run a new DCA with the extra sample
DCANewSample <- decorana(SpeciesNewSample)
DCANewSample
# plot the DCA
ordiplot(DCANewSample, type="text", main="DCA with new sample Chenopodium album")
# figure out the column number for Juncbufo
which(colnames(dune)=="Juncbufo")
# create a new data frame with a new sample containing Juncbufo with a cover of 5
NewSample <- c(rep(0,15), 5, rep(0,10))
SpeciesNewSample <- data.frame(rbind(dune, NewSample))
# run a new DCA with the extra sample
DCANewSample <- decorana(SpeciesNewSample)
DCANewSample
# plot the DCA
ordiplot(DCANewSample, type="text", main="DCA with new sample of Juncbufo")
```



<br>

***

### <span style="color:cornflowerblue">Question 8: Principal Coordinates Analysis (PCoA) </span>
Principal Coordinates Analysis (PCoA), also known as Multidimensional Scaling (MDS), is a generalization of the ordination techniques we have already seen as it can be estimated with any distance matrix. In contrast, PCA is restricted to Euclidean distances while CA preserves Chi-Square distances.
Here, we will explore PCoA on four different distance matrices: 
(i)   Bray-Curtis
(ii)  Jaccard
(iii) Euclidean
(iv)  Mahalanobis

The R base statistics library offers the routine cmdscale() for performing MDS. There are numerous other libraries offering PCoA analysis

#### <span style="color:forestgreen"> After performing the four PCoA analyses, please answer the following questions:
(i) 	How similar are the ordination plots from the four distance matrices?
(ii)	Why are some plots more similar than others?
(iii)	Which PCoA plot is most similar to the output from a PCA?
(iv)	Which PCoA plot is most similar to the output from a CA? </span>
<br>
```{r}
# generate the four distance matrices
dune.bray.distance.matrix<-vegdist(dune,method="bray")
dune.jaccard.distance.matrix<-vegdist(dune,method="jaccard")
dune.euclidean.distance.matrix<-vegdist(dune,method="euclidean")
dune.mahalanobis.distance.matrix<-vegdist(dune,method="mahalanobis")
#
# what can be accomplished with the MDS routine?
#
help(cmdscale)
#
# run an MDS using cmdscale() for each of the four distance matrices
#
dune.bray.location<-cmdscale(dune.bray.distance.matrix)
dune.jaccard.location<-cmdscale(dune.jaccard.distance.matrix)
dune.euclidean.location<-cmdscale(dune.euclidean.distance.matrix)
dune.mahalanobis.location<-cmdscale(dune.mahalanobis.distance.matrix)
#
# now take a look at some plots
#
plot(dune.bray.location, xlab="PCoA 1", ylab="PCoA 2")
title(main="PCoA on Bray Curtis distances of dune sites")
text(dune.bray.location,labels=row.names(dune.bray.location),pos=2)
#
plot(dune.jaccard.location, xlab="PCoA 1", ylab="PCoA 2")
title(main="PCoA on Jaccard distances of dune sites")
text(dune.jaccard.location,labels=row.names(dune.jaccard.location),pos=2)
#
plot(dune.euclidean.location, xlab="PCoA 1", ylab="PCoA 2")
title(main="PCoA on Euclidean distances of dune sites")
text(dune.euclidean.location,labels=row.names(dune.euclidean.location),pos=2)
#
plot(dune.mahalanobis.location, xlab="PCoA 1", ylab="PCoA 2")
title(main="PCoA on Mahalanobis distances of dune sites")
text(dune.mahalanobis.location,labels=row.names(dune.mahalanobis.location),pos=2)
```

***
<br>

### <span style="color:cornflowerblue">Question 9:	Non Metric Multidimensional Scaling </span>

#### <span style="color:forestgreen"> At the conclusion of this exercise, you should be able to:
(i)	State which metrics are supported in the vegan implementation of NMDS
(ii)	Evaluate the appropriate number of dimensions for the ordination
(iii)	Evaluate the consequences of using different distance metrics
(iv)  Compare the vegan default NMDS to the results of a CA and a DCA </span>

```{r ex8, inclue=TRUE, warning=FALSE, message=FALSE}
# run NMDS with the defaults and 2 dimensions, what is the stress?
#
dune.NMDS.bray.2<-metaMDS(dune,k=2)
#
# run NMDS with the defaults and 3 dimensions, what is the stress ?
#
dune.NMDS.bray.2<-metaMDS(dune,k=3)
#
# what happens when Euclidean distances are used?
#
dune.NMDS.euclidean.2<-metaMDS(dune,k=2)
#
# plot some results
#
ordiplot(dune.NMDS.bray.2, type="text", main="NMDS from Bray Curtis distance, 2 Dimensions")
ordiplot(dune.NMDS.euclidean.2, type="text", main="NMDS from Euclidean distance, 2 Dimensions")
#
# create Shepard plots to show how the stress evolves
#
stressplot(dune.NMDS.bray.2)
stressplot(dune.NMDS.euclidean.2)
#
# generate the CA and DCA results
#
dune.CA<-cca(dune)
dune.DCA<-decorana(dune)
#
# plot the CA and DCA results
#
ordiplot(dune.CA, type="text", main="CA on Dune Meadow data")
ordiplot(dune.DCA, type="text", main="DCA on Dune Meadow data")


```

<br>

***


### <span style="color:cornflowerblue">Question 10: Data Quality Assessment</span>
These are only a few examples on how to assess data quality. 

#### <span style="color:forestgreen">Is there a pattern in the distribution of missing values that may lead to the omission of a species or a sample? Is Grubbs test a good method for finding outliers in a multivariate data set? Would you say that the original Dune Meadow data set is OK for further analyses? Motivate your answer!</span>



```{r ex10, inclue=TRUE, message=FALSE, warning=FALSE}
library("vegan") 
library("reshape2")
library("outliers") # For Grubbs test
library("gdata") # for unmatrix
library("VIM") # For matrixplot

data(dune)

# Create some outliers and missing values in a temporary dataset called mvo.dune (acronym for MultiVariate Outlier)
mvo.dune<-as.matrix(dune)
n_outl<- 5 # Give number of outliers to be created, here 5
v_outl<-30 # Give value of the outlier(s), here 30
mvo.dune[sample(1:length(mvo.dune),n_outl, replace = FALSE)]<-v_outl # Replace some data with outliers, at random positions
mvo.dune[sample(1:length(mvo.dune),n_outl, replace = FALSE)]<-NA # Replace some data with NAs, at random positions

# Cleveland plot to inspect data
dotchart(as.matrix(mvo.dune), lcolor=col(mvo.dune), main="Cleveland plot", xlab="Observerd value")

# Find data > 9, as no value should be higher than 9 in the dune meadow data set
(GT9 <- which(mvo.dune > 9, arr.ind=TRUE))

# Find missing values
(mvo_miss<-which(is.na(mvo.dune), arr.ind=TRUE))

# Two ways of visualising missing data, using the package "VIM"
aggr(as.matrix(mvo.dune)) 
matrixplot(mvo.dune) 
```
<br>
We can also try testing for outliers in the data. Be aware that this is only indicative, and you still need to exercise your judgement about what should be excluded as an outlier!

```{r}
# To scan all variables with Grubbs test for outliers
# Note: Only indicative!
mvo.dune_melt<-melt(mvo.dune) # Restructure data
a<-as.data.frame(do.call("rbind", with(mvo.dune_melt, tapply(value, Var1, function(x) 
  unlist(grubbs.test(x)[c("statistic", "p.value", "alternative")])))))
(a[which(a$p.value < 0.05),])

# Indicative search for outliers in a multivariate data set. 
# Multiple peaks and high SD indicate outliers may occur
dist1<-vegdist(mvo.dune, method="euclidean", binary=FALSE, diag=FALSE, upper=FALSE, na.rm = TRUE) 
d1<-data.frame(unmatrix(as.matrix(dist1), byrow = TRUE))
d1r<-t(data.frame(strsplit(row.names(d1), ":")))
d1rr<-data.frame(d1,d1r)
d1mean<-tapply(d1rr$unmatrix.as.matrix.dist1.., d1rr$X1, mean)
sd(d1mean) # Standard deviation of the mean distances. If > 3: strong outlier(s)
plot(stats::density(d1mean), col = "red", xlab="Distance between observations", main = "Indictive test of multivariate outliers\nData with noise")
legend("topright",paste("SD =",round(sd(d1mean),1))) # Legend with standard deviation

# Same as above but on the original dune meadow dataset (i.e. without outliers)
dist1<-vegdist(dune, method="euclidean", binary=FALSE, diag=FALSE, upper=FALSE, na.rm = TRUE) 
d1<-data.frame(unmatrix(as.matrix(dist1), byrow = TRUE))
d1r<-t(data.frame(strsplit(row.names(d1), ":")))
d1rr<-data.frame(d1,d1r)
d1mean<-tapply(d1rr$unmatrix.as.matrix.dist1.., d1rr$X1, mean)
sd(d1mean) # Standard deviation of the mean distances. If > 3: strong outlier(s)
plot(stats::density(d1mean), col = "red", xlab="Distance between observations",main = "Indictive test of multivariate outliers\nOriginal Dune Meadow data")
legend("topright",paste("SD =",round(sd(d1mean),1))) # Legend with standard deviation

```

***


<br>

### <span style="color:cornflowerblue">Question 11:	Do an RDA and a CCA on the Dune Meadow data set, using the full set of environmental variables.  </span>

#### <span style="color:forestgreen"> Compare the ordination diagrams and numerical results! What does the significance test tell you? When only looking at the results, which method explains the variation best? Taking the nature of the data into account, which method should be used?  </span>

```{r ex11, inclue=TRUE, warning=FALSE, message=FALSE}
# RDA 
dune.rda <- rda(dune ~ . , dune.env)
summary(dune.rda)
anova(dune.rda)

# CCA 
dune.cca <- cca(dune ~ . , dune.env)
summary(dune.cca)
anova(dune.cca)

#plots
ordiplot(dune.rda, type="text", main="RDA")
ordiplot(dune.cca, type="text", main="CCA")
```

<br>

***

<br>

### <span style="color:cornflowerblue">Question 12:	Use the results from the CCA ordination in exercise 11 to look at and interpret the variance inflation factor, and to look for warnings about outliers in the explanatory data!   </span>

#### <span style="color:forestgreen"> Look at and interpret the inflation factors for each environmental variable! Are there any extreme values? </span>

```{r ex12, inclue=TRUE, warning=FALSE, message=FALSE}
# VIF for CCA 
dune.cca <- cca(dune ~ . , dune.env)
vif.cca <- vif.cca(dune.cca)
vif.cca
```
Function vif.cca gives the variance inflation factors for each constraint or contrast in factor constraints. In partial ordination, conditioning variables are analysed together with constraints. Variance inflation is a diagnostic tool to identify useless constraints. A common rule is that values over 10 indicate redundant constraints. 

<br>

***

<br>

### <span style="color:cornflowerblue">Question 13:	Do a CA on the Dune Meadow data set, with the full set of environmental variables included in the analysis, and compare with a CCA on the same data sets. </span>

#### <span style="color:forestgreen"> Compare the constrained and unconstrained ordinations! What is the difference between the methods? When could an unconstrained ordination with explanatory variables be a good option? </span>


```{r ex13, inclue=TRUE, warning=FALSE, message=FALSE}
# CA with env. variables 
fit <- envfit(dune.ca, dune.env, perm=999)
fit
ordiplot(dune.ca,  type="text", main="CA with env. variables fitted after")
plot(fit)

#CCA 
dune.cca <- cca(dune ~ . , dune.env)
dune.cca
ordiplot(dune.cca, type="text", main="CA")
```

<br>

***

### <span style="color:cornflowerblue">Question 14:	Site scores can be calculated in two ways; Weighted Average and Linear Combination. Here you will do both and compare the results. Both options are available in most software, but the default varies.</span>

#### <span style="color:forestgreen"> What is the main difference between the two ways. How to know which method to select? What does the display = "cn" in the last plots mean?</span>

```{r}
#### Illustration of difference between WA and LC scores

data("dune")
data("dune.env")
dune.env$Moisture<-as.numeric(dune.env$Moisture) #Set Moisture to numeric for the illustration of WA and LC

# Make a CCA with Management and Moisture as explanatory variables
cca.dune<-cca(dune~Management + Moisture, dune.env)
# Look at the result
anova(cca.dune)
summary(eigenvals(cca.dune))

# Make two plots, one with weighted average (WA) and one with Linear Combination (LC) to calculate the site scores. 
par(mfrow= c(1,2)) # To get two plots beside each other
plot(cca.dune, type = "n", display = c("wa"), scaling = "sites", main = "WA", pch = 3)
points(cca.dune, display = c("wa"), scaling = "sites", pch = 3)
text(cca.dune, display = "wa", scaling = "sites", pos = 2)
ordispider(cca.dune, dune.env$Management, col = "black", label= TRUE, scaling = "sites", display = "wa")

plot(cca.dune, type = "n", display = c("lc"), scaling = "sites", main = "LC", pch = 3)
points(cca.dune, display = c("lc"), scaling = "sites", col = "red", pch = 19)
text(cca.dune, display = "lc", col = "red", scaling = "sites", pos = 3)
ordispider(cca.dune, dune.env$Management, col = "red", label= TRUE, scaling = "sites", display = "lc")
par(mfrow= c(1,1))

# Same as above, but here with sites and species
par(mfrow= c(1,2))
plot(cca.dune, display = c("sp", "wa", "cn"), scaling = "species", main = "Weighted average")
plot(cca.dune, display = c("sp", "lc", "cn"), scaling = "species", main = "Linear combination")
par(mfrow= c(1,1))

```

<br>

### <span style="color:cornflowerblue"> Question 15:	Next, you should investigate whether the four different management regimes (SF standard farming, BF biodynamical farming, HF hobby farming, and NM nature management) have any effect on the species distribution. This is done by using these four management types as environmental variables in a constrained ordination.  </span>

#### <span style="color:forestgreen"> Inspect and interpret the results of the permutations and graph! Compare with the CCA on the full set of explanatory data in exercise 13. </span>

To investigate if the vegetational pattern is an effect of management, or if the pattern is random, Monte Carlo permutations will be used. 


```{r ex15, inclue=TRUE, warning=FALSE, message=FALSE}
# CCA using only Management as the environmental variable
dune.ccaMN <- cca(dune ~ Management , dune.env.original)
summary(dune.ccaMN)
# Permutation test on CCA
MCperm <- permutest(dune.ccaMN, permutations=999)
MCperm
# Plot the ordination with only Management
ordiplot(dune.ccaMN, type = "points", main = "CCA only Management")
ordihull(dune.ccaMN,groups = dune.env.original$Management,draw = "polygon",col = "grey70", label = T)
```

<br>

***

<br>


### <span style="color:cornflowerblue">Question 16:	Next, you want to investigate whether initial soil characteristics (A1 and moisture) also are important for differences in species composition between fields? To do this, do another CCA with A1 and Moisture as environmental variables and perform a significance test.   </span>

#### <span style="color:forestgreen"> Explain the results of the permutations! Compare the results in exercises 16 and 17! </span>

```{r ex16, inclue=TRUE, warning=FALSE, message=FALSE}
# CCA using A1 and moisture as the environmental variable
dune.ccaA1ms <- cca(dune, dune.env[,1:2])
summary(dune.ccaA1ms)
# Permutation test on CCA
MCpermA1ms <- permutest(dune.ccaA1ms, permutations=999)
MCpermA1ms 
# Plot the ordination
ordiplot(dune.ccaA1ms, type="text", main="CCA A1 and moisture")

```

<br>

***

<br>

## Direct methods with testing of environmental variables

### <span style="color:cornflowerblue"> Question 17: Partial ordination    </span>

Now, you have concluded that both soil characteristics and management type are determining the species composition significantly. But, what if the observed differences between management types are not caused by management type but by initial differences in soil characteristics? To investigate this, you have to test whether there still is a difference in vegetation between management types, after accounting for (i.e. removing) the effects of soil characteristics.
This is done by a partial CCA ordination. Partial ordinations are used to eliminate effects of selected variables by specifying them as covariates. In this case, use the two soil characterising variables (A1 and Moisture) as covariates, and the four management types as environmental variables. 


#### <span style="color:forestgreen">What does the permutation test tell you? Also, compare the results with the results from the permutation test on only management types!  </span>

```{r ex17, inclue=TRUE, warning=FALSE, message=FALSE}
#This partials out the effect of A1 and Moisture before analysing the effects of management
MN.cca <- cca(dune ~ Management + Condition(A1 + Moisture), data=dune.env.original)
summary(MN.cca)
#permutation test
permutest(MN.cca, permutations=999)
# Plot the ordination
ordiplot(MN.cca, type = "points")
ordihull(MN.cca,groups = dune.env.original$Management,draw = "polygon",col = "grey70", label = T)
```

<br>

***

<br>

### <span style="color:cornflowerblue"> Question 18: The amount of applied manure is a third important variable given in the environmental data set associated with the dune meadow data set. Repeat exercise 17, but with manure added to the set of covariates. </span>


#### <span style="color:forestgreen"> Inspect and interpret the results of the permutations! </span>

```{r ex18, inclue=TRUE, warning=FALSE, message=FALSE}
#This partials out the effect of A1, Moisture, and Manure before analysing the effects of management
MN.cca2 <- cca(dune ~ Management + Condition(A1 + Moisture + Manure), data=dune.env.original)
summary(MN.cca2)
#permutation test
permutest(MN.cca2, permutations=999)
```

<br>

***

<br>

### <span style="color:cornflowerblue"> Question 19: Forward selection </span>

#### <span style="color:forestgreen"> We have now concluded that management type is of subordinate importance. Now, you want to test the importance or significance of individual environmental variables. This is done by Forward selection. What is the effect of Forward selection? In what way do these results differ from the results obtained when the whole Environmental data table was used as explanatory variables? Were the set of explanatory variables selected by the Forward selection the best set? Why would one perform a p-value correction?  </span>

```{r ex19, echo=TRUE, message=FALSE, warning=FALSE, inclue=TRUE}
# A first CCA with all environmental variables
cca.dune.all <- cca(dune ~ ., data = dune.env) # the dot after the tilde (~) means "include all from data"
anova(cca.dune.all) # Test overall significance of the explanatory variables. If not significant no need to proceed with step wise selection
adjR2.cca.dune <- RsquareAdj(cca.dune.all)$adj.r.squared # How much the adjusted R2 explains when all variables in dune.env are included
adjR2.cca.dune

# A null model with only intercept, needed for the step wise selection
cca.dune.0 <- cca(dune ~ 1, data = dune.env) # model containing only species matrix and intercept

# The step wise selection, based on R2
sel.env.dune <- ordiR2step(cca.dune.0, scope = formula (cca.dune.all), R2scope = adjR2.cca.dune, direction = 'forward', permutations = 999)
sel.env.dune
sel.env.dune$anova

```
Since there is (a potentially high) number of tests of significance during the forward selection procedure, 
it is better to apply a correction for multiple testing issue. Here we apply the Holm correction.
```{r}
sel.env.dune_adj <- sel.env.dune
sel.env.dune_adj$anova$`Pr(>F)` <- p.adjust(sel.env.dune$anova$`Pr(>F)`, method = 'holm', n = ncol(dune.env)) # 7 other methods available
sel.env.dune_adj$anova
```

<br>

***

<br>

### <span style="color:cornflowerblue">Question 20: Interpreting ordination results using species traits </span>

#### <span style="color:forestgreen"> Experienced plant ecologists may already have looked at the species in the Dune Meadow ordination graphs and concluded that species with similar traits occur together. This is possible if you have good knowledge about plant species ecological preferences, and if there are relatively few species in your dataset. In this exercise we will use tabulated data on species ecological preferences (the Ellenberg indicator values) to interpret the results of the ordinations. Which Ellenberg values are most important for the distribution of the species? What do the different axes represent in terms of environmental gradients? </span>

```{r message=FALSE, warning=FALSE}
#load Ellenberg data
dune.ell <- readRDS("Ellenberg.RDS")
dune.mean.ell <- readRDS("Mean_Ellenberg.RDS")
#The vegan implementation of forward selection of variables does not accept NA values.
#To get around this we will replace the two NAs with the mean value for all plots in that column.
#Do not do this in a real analysis without seriously thinking about how it affects your results!
## Code to replace missing values with the column mean, and save the results as a new dataframe
## called dune.mean.ell.impute (since we are imputing the missing values).
dune.mean.ell.impute <- data.frame(
    sapply(
        dune.mean.ell,
        function(x) ifelse(is.na(x),
            mean(x, na.rm = TRUE),
            x)))
#CCA analysis
#create global model with CCA (including all variables) and test it's significance, and if it is significant,
#we use the ordistep function with appropriate arguments to do forward selection of variables.
cca1 <- cca(dune ~ ., data = dune.mean.ell.impute) # full model (with all explanatory variables)
anova(cca1) #overall model is significant
# Start by making ordinations
cca0 <- cca (dune~ 1, data = dune.mean.ell.impute) # empty model only with intercept
cca1 <- cca(dune ~ ., data = dune.mean.ell.impute, na.action = na.omit) # full model (with all explanatory variables)
cca1
plot(cca1, main="CCA, all data")
#Stepwise approach, using "ordistep"
step2<-ordistep(cca0, scope = formula(cca1), direction="forward")
step2$anova # which three are most significant in the final selection?
```

#### 

<br>

***

<br>



### <span style="color:cornflowerblue">Question 21: Decomposition of variance </span>

#### <span style="color:forestgreen"> Perform the analyses and interpret the results! How much of the total variation (% of All) is explained by: - uniquely by Management (effect of Soil removed), - uniquely by Soil  (effect of Management removed), - jointly by Soil and Management (the interaction) - Not by Soil or Management? </span>

In usual analysis of variance experiments, the variance is decomposed into components. The same can be done in multivariate analyses. In this exercise you will decompose the variance in the Dune meadow data set into different variance components. You will use two groups of variance components: Group 1 is Management and Group 2 is the soil variables (A1 and Moisture). In ordinations, the variance is expressed by the sum of the eigenvalues. 

The result gives you the fraction of the total variation that is explained by:
    a) uniquely by Management (effect of soil removed),
    b) uniquely by soil (effect of Management removed),
    c) jointly by Soil and Management (the interaction).
    
The total explained variation is the sum of a), b) and c). 

The total variation in the dataset is the sum of all unconstrained eigenvalues.


```{r, warning=FALSE, message=FALSE}
data("dune")
data("dune.env")
#as we did in Question 3:PCA, we need to reformulate the environmental data to make
#new columns for each of the management methods.
dummy_management <- as.data.frame(model.matrix( ~ Management - 1, data=dune.env )) 
#add these to the dataset
dune.env <- dune.env %>% select(A1, Moisture, Manure, Use) %>% cbind(.,dummy_management) 
dune.env$Moisture <- as.numeric(as.character(dune.env$Moisture)) #make numeric
dune.env$Manure <- as.numeric(as.character(dune.env$Manure))
dune.env$Use <- as.numeric(dune.env$Use)
#make column names shorter
dune.env <- dune.env %>% rename(BF = ManagementBF, HF = ManagementHF,
                                NM = ManagementNM, SF = ManagementSF)
## variance partitioning
management <- dune.env[,c("BF", "HF", "NM", "SF")]
soil <- dune.env[,c("A1", "Moisture")]
# examine the explanatory variable of each class of variables.
varp <- varpart(dune, management, soil)
varp
plot (varp, digits = 2, Xnames = c('Management', 'Soil'), bg = c('red', 'blue'))
```

<br>

***

<br>

### <span style="color:cornflowerblue">Question 22: Analysing a time series with vegetation data </span>

#### <span style="color:forestgreen"> Is there a significant change in species composition over time?  What is the effect of defining Plots as cofactors? </span>

Quite often vegetation ecologists have data from repeated inventories in permanent plots. The overall question to answer is if there has been a significant and directional change in the species composition. In this exercise we have rearranged the Dune Meadow data so that it consists of only 10 plots, each analysed 2 times (same species as in all other exercises, just grouping and rearrangement of plots). We want to analyse if there has been a consistent change in species composition between the two inventories. We are thus not interested in differences between the ten plots. 

```{r, warning=FALSE, message=FALSE}
#This partials out the effect of Plot before analysing the effects of Time
#First load the time series versions of the data
SpeTS <- readRDS("SpeTS.RDS")
EnvTS <- readRDS("EnvTS.RDS")
time.cca <- cca(SpeTS ~ Time + Condition(Plot), data=EnvTS)
time.cca
treat <- EnvTS$Time #
colvec <- c("red2", "green4") #set colours to be applied to different levels of factor "Time"
plot(time.cca, type = "n", display = c("lc"))#plots the axes
with(time.cca, points(time.cca, display = c("lc"), col = colvec[treat],#plots the points
                      pch = 21, xlim = c(-2,2), bg = colvec[treat]))
```

```{r}
#permutation test
with(EnvTS, anova(time.cca, by="term", perm=500, strata=Plot))
```


```{r, warning=FALSE, message=FALSE}
#Compare with results for time as only explanatory variable and no cofactors
time2.cca <- cca(SpeTS ~ Time, data=EnvTS)
time2.cca
#permutation test
with(EnvTS, anova(time2.cca, by="term", perm=500))
```


```{r, warning=FALSE, message=FALSE}
## plot ellipsoid hulls
treat <- EnvTS$Time
plot(time2.cca, type = "n")#plots the axes
with(time2.cca, points(time2.cca, col = colvec[treat],#plots the points
                      pch = 21, bg = colvec[treat]))
ordihull(time2.cca,groups=treat,draw="polygon",col="grey70",label=T)#draw hulls around area of each level of factor "Time"
```

<br>

***


<br>

### <span style="color:cornflowerblue">Question 23: A multivariate Before-After-Control- Impact (BACI) study </span>

#### <span style="color:forestgreen"> Did the treatment have an effect? What was permuted? </span>


In this exercise we will continue to use the rearranged Dune Meadow data from exercise 24. A difference is that the plots are now divided into four groups:
    1. Control plots before a treatment (i.e. not treated)
    2. Control plots after a treatment (i.e. not treated)
    3. Impact plots before treatment (i.e. not treated)
    4. Impact plots after treatment (this is where the treatment is made)

The four groups are indicated in variable Treat in the environmental data set (EnvTS).
The question you want to answer is if the treatment caused a change in species composition that is significantly different from the change in the control plots.


```{r, warning=FALSE, message=FALSE}
baci.cca <- cca(SpeTS ~ Treat + Condition(Plot + Time), data=EnvTS)
baci.cca
```



```{r, warning=FALSE, message=FALSE}
with(EnvTS, anova(baci.cca, by="term", perm=500, strata=Treat))
#Here with() is a special function that makes variables in dune.env visible to
#the following command. If you only type Moisture in an R prompt, you will get
#an error of missing variables
```




```{r, warning=FALSE, message=FALSE}
## plot results
treat <- EnvTS$Treat
ordiplot(baci.cca,type="points")
ordihull(baci.cca,groups=treat,draw="polygon",col="grey70",label=T)#draw hulls around area of each level of factor "Time"
```


<br>

***
<br>

### <span style="color:cornflowerblue">Question 24:	Comparing ordinations </span>

#### <span style="color:forestgreen"> Questions:
(i)  How similar is the arrangemont of sites in ordination space based on plant and insect data?
(ii) Do the three analyses (Procrustes, Co-Correspondence and Mantel) indicate similar odinations of sites based on plant vs. insect data?
 </span>

These are two ways of comparing ordinations or the structure in related multivariate datasets. They are used to compare how objects differ when they are described by different sets of descriptors, or if there are repeated samplings of the same objects using the same descriptors.

```{r ex24, inclue=TRUE, warning=FALSE, message=FALSE}
library(cocorresp)
library(vegan)
data(dune)
#
# DCA on the plant species data 
#
plants.DCA <- decorana(dune)
#
# DCA on the insect data (called Bugs)
#
insects <- readRDS("Bugs.RDS")
insects.DCA <- decorana(insects)
#
# Have a look at the DCA based on insects
#
insects.DCA
#
# perform a Procrustes analysis on the two DCAs from different datasets
#
plants.and.insects.from.DCA <- procrustes(plants.DCA, insects.DCA)
plot(plants.and.insects.from.DCA, kind=1, type="text")
#
# Labels show the position of the samples in the second ordination, and arrows point to their positions in the
# target ordination Function protest tests the non-randomness (`significance') between two configurations
#
ProCsig <- protest(plants.DCA, insects.DCA)
ProCsig
#
# Co-correspondence analysis
#
CoCor <- coca(dune ~ ., data = insects, method = "symmetric")
summary(CoCor)
corAxis(CoCor)
#
# set up and perform the Mantel test
#
insects.distance<-vegdist(insects)
plants.distance<-vegdist(dune)
#
# what does mantel() do?
#
help(mantel)
#
# perform the mantel test using parametric and non-parametric correlations
mantel(insects.distance,plants.distance)
mantel(insects.distance,plants.distance,method="spear")
```


<br>

***


<br>



### <span style="color:cornflowerblue">Question 25 : Classification 1</span>

Start by testing the non-hierarchical K-means clustering, using Multivar / K-Means. Note that it is
recommended that K-means need at least 100 observations (500 according to some sources) to be reliable!
Let us ignore the sample size issue for the moment, and ask for 4 clusters (for later comparison with the
four management types). Test different hierarchical agglomeration algorithms and similarity indices. Use
at least the Euclidian and the Bray-Curtis similarity measures for the hierarchical clustering technique.
Also try Ward's method 

#### <span style="color:forestgreen"> # Questions:
(i) 	How many clusters seem to exist in the Dune data set?
(ii)	Is a 3- or 4-cluster solution a better match to the management type in the dune.env dataset?
(iii) Do the two approaches to hierachical clustering reveal similar patterns in the Dune data set?
(iv) Interpret the Hopkins statistic for the Dune data set </span>




```{r, warning=FALSE, message=FALSE}
#
library("clustertend")
library("factoextra")
library("NbClust")
#
# Have a look to see what NbClust does
#
help(NbClust)
#
# make a distance matrix using the Bray-Curtis method
#
dune.dist <- vegdist(dune, method = "bray")
#
# see how many clusters there appear to be in the data
#
res<-NbClust(dune, diss=dune.dist, distance = NULL, 
	min.nc=2, max.nc=12, method = "kmeans", index = "kl") 
	
library(dendextend)
#
# K-Means Cluster Analysis based on the previously identified number of clusters
#
dune.fit.3 <- kmeans(dune, 3, nstart = 50) 
#
# As the final result of k-means clustering result is sensitive to the random starting assignments, 
# we specify nstart = 50. This means that R will try 50 different random starting assignments and then
# select the best results corresponding to the one with the lowest within cluster variation.
# Note that k-means clustering involves randomness so you won't neccessarily get exactly the same results 
# if you repeat it.
#
# There are also several different algorithms that can be used in k-means clustering, and different software 
# have can use different defaults.Check out the help file for the kmeans function if you want to know more
# about these.
#
#cluster number for each data point
#
dune.fit.3$cluster
#
#cluster sizes
#
dune.fit.3$size
#
#compare clusters to management category
#
table(dune.fit.3$cluster, dune.env.original$Management)
#
# there are four management categories, try k-means clustering with k=4
#
dune.fit.4 <- kmeans(dune, 4, nstart = 50) 
#
#cluster number for each data point
#
dune.fit.4$cluster
#
#cluster sizes
#
dune.fit.4$size
#
#compare clusters to management category
#
table(dune.fit.4$cluster, dune.env.original$Management)
#
#try some approaches to hierarchical clustering
#
dend2 <- dune %>% # data
        dist(method = "euclidean") %>% # calculate a distance matrix, choose method 
        hclust(method = "ward.D") %>% # Hierarchical clustering, choose method 
        as.dendrogram # Turn the object into a dendrogram.
dend2 %>% set("labels_color") %>% set("branches_lwd") %>%  set("branches_k_color", k = 4) %>% plot
#
#the dist() function does not include Bray-Curtis but we have already made a distance matrix using 
#the vegdist() function from vegan.
#
dend3 <- dune.dist %>% # data (Bray-Curtis)
        hclust(method = "aver") %>% # Hierarchical clustering, choose method 
        as.dendrogram # Turn the object into a dendrogram.
dend3 %>% set("labels_color") %>% set("branches_lwd") %>%  set("branches_k_color", k = 4) %>% plot
#
#we can colour the labels based on their grouping in the kmeans analysis to compare clusters
#
dend3 %>% set("labels_color", dune.fit.3$cluster) %>% set("branches_lwd") %>%  set("branches_k_color", k = 4) %>% plot
#
#tanglegram is a nice function to compare dendrograms that you might want to try out as an extra
#
tanglegram(dend2, dend3)
#
# finally, should the Dune data be clustered at all?
#
library(clustertend)
#
# Compute Hopkins statistic for the Dune dataset
#
set.seed(123)
#
hopkins(dune, n = nrow(dune)-1)
```

<br>

***

### <span style="color:cornflowerblue">Question 26: Classification 2: Partition and Regression Trees </span>

Most of the code is taken from 
https://jonlefcheck.net/2015/02/06/a-practical-guide-to-machine-learning-in-ecology/
Here, we will use the iris data. This is a "classic" data set used for exploring multivariate 
methods for discrimination.https://en.wikipedia.org/wiki/Iris_flower_data_set

#### <span style="color:forestgreen">When you have completed the exercise, you should be able to answer the following questions:
(i)	Based on a visual analysis, which dimensions provide the best separation between iris species?
(ii)	Are your insights from the visual analysis confirmed by the partition tree analysis?
(iii)	Does the partition tree provide a perfect species identification or are there misclassifications? </span>

```{r}
# Before starting the analysis, we will generate pairwise scatterplots of the data
#
# the following line will first check to see inf the "psych" library is loaded, and only 
# load it if it is not present
#
if(!is.element("psych",installed.packages()[,1])) {install.packages("psych") }
library(psych)
library(rpart)
#
# create pairwise plots of the variables in the iris data set
#
pairs.panels(iris[,-5],gap=0,bg=c("red","blue","yellow")[iris$Species],pch=21)
#
# what does rpart() do ?
#
help(rpart)
#
# set up the partition tree model for the iris data
#
iris.tree.model<-rpart(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data = iris)
# 
# display the partition tree
#
plot(iris.tree.model)
text(iris.tree.model)
#
# plots for confirmatory visual analysis
#
plot(iris$Petal.Length, col = c("red", "blue", "forestgreen")[iris$Species], xlab = "", ylab = "Petal Length (cm)")
legend(120, 2, c("setosa","versicolor","virginica"), col = c("red", "blue", "forestgreen"), pch = 1)
abline(h = 2.45)
#
plot(iris$Petal.Width, col = c("red", "blue", "forestgreen")[iris$Species], xlab = "", ylab = "Petal Width (cm)")
legend(20, 1.5, c("setosa","versicolor","virginica"), col = c("red", "blue", "forestgreen"), pch = 1)
abline(h = 1.75)
```
***

### <span style="color:cornflowerblue">Question 27: Principal Response Curves</span>
In the following exercise, we will attempt to identify temporal structure in a data set in two ways.
First, we will perform an RDA against time, we will then perform a more appropriate analysis using 
Principal Response Curves (PRC).
#### <span style="color:forestgreen"> At the conclusion of this exercise, you should be able to state:
(i) Was there an effect of dose over time on the invertebrate community?
(ii) Which species changed most under the highest dose compared to the control ? </span>

```{r}
# We will use the pyrifos data set, which we have not previously seen.
#
help(pyrifos)
#
# As you will have seen from the description of the pyrifos data set, we need to do some data manipulation
# before it will be possible to perform any analysis.
#
data(pyrifos)
ditch_ <- gl(12, 1, length=132)
week_ <- gl(11, 12, labels=c(-4, -1, 0.1, 1, 2, 4, 8, 12, 15, 19, 24))
dose_ <- factor(rep(c(0.1, 0, 0, 0.9, 0, 44, 6, 0.1, 44, 0.9, 0, 6), 11)) 
#
# first, let's see if there is temporal structure in the data by performing an RDA with time as the 
# predictor variable
#
pyrifos.RDA.week<-rda(pyrifos ~ week_)
summary(pyrifos.RDA.week)
#
# Display the ordination plot to see if there may be interpretable temporal structure
#
pyrifos.RDA.week.plot<-ordiplot(pyrifos.RDA.week)
text(pyrifos.RDA.week.plot,"centroids", col="blue",pos=2,cex=2)
#
# now can we say anything about dose effects?
#
pyrifos.RDA.dose<-rda(pyrifos ~ dose_)
summary(pyrifos.RDA.dose)
#
# Display the ordination plot to see if there is interpretable structure related to dose
#
pyrifos.RDA.dose.plot<-ordiplot(pyrifos.RDA.dose)
text(pyrifos.RDA.dose.plot,"centroids", col="blue",pos=2,cex=2)
#
# Now perform the appropriate analysis for exploring temporal structure
#
pyrifos.PRC<-prc(pyrifos, treatment=dose_, time=week_)
summary(pyrifos.PRC)   
#
# plot some results
#
plot(pyrifos.PRC)
#
# Plotting the total result set is a bit messy, plot only the common species based on sum of
# abundances
#
pyrifos.SumOfAbundances<-colSums(pyrifos)
plot(pyrifos.PRC,select=pyrifos.SumOfAbundances > 100)
logabu <- colSums(pyrifos)
plot(pyrifos.PRC, select = logabu > 100)
#
## Ditches are randomized, we have a time series, and are only
## interested in the first axis
pyrifos.PRC.ctrl<- how(plots = Plots(strata = ditch_,type = "free"),
	within = Within(type = "series"), nperm = 99)
anova(pyrifos.PRC, permutations = pyrifos.PRC.ctrl, first=TRUE)
```
***

### <span style="color:cornflowerblue">Question 28: Differences between groups. ANOSIM, adonis and SIMPER</span>

#### <span style="color:forestgreen"> What does the results from anosim and adonis tell you? Give one other example of predefined groups that could be uses as the grouping variable(s)! </span>

ANOSIM (Analysis Of Similarities) is a non-parametric test of significant difference between two or more groups, based on any distance measure. In this case, use the clusters from the K-means exercise. Change to Bray-Curtis similarity index.

ANOSIM gives you the P value and an R value. R value close to 1 indicates high separation between levels of your factor while R value close to 0 indicate no separation between levels of your factor.

Try also the recommended alternative "adonis" - Analysis of variance using distance matrices - for partitioning distance matrices among sources of variation and fitting linear models (e.g., factors, polynomial regression) to distance matrices; uses a permutation test with pseudo-F ratios.
```{r}
# Start with a CA with Management as environmental factor and hulls around the different management types
data("dune")
data("dune.env")
dune.ca<-cca(dune)
fit.mgm<-envfit(dune.ca~Management, dune.env, perm = 0)
plot(dune.ca, type = "n", scaling = "symmetric")
with(dune.env, points(dune.ca, display = "sites", scaling = "symmetric", col = as.numeric(Management), pch=16))
with(dune.env, ordispider(dune.ca, Management, scaling = "symmetric", col="skyblue"))
with(dune.env, ordihull(dune.ca, Management, scaling = "symmetric",label = TRUE))

dune.dist <- vegdist(dune, method = "bray") #create distance matrix based on Bray-Curtis method
dune.ano <- anosim(dune.dist, dune.env$Management) #Comparing groupings based on management
summary(dune.ano)
plot(dune.ano, xlab = "Anosim, Dune meadow management types")

#ANOSIM gives you the P value and a R value. R value close to 1 indicates high separation between levels of your factor while R value #close to 0 indicate no separation between levels of your factor.

# Try also the recommended alternative "adonis" - Analysis of variance using distance matrices - for partitioning distance matrices among sources of variation and fitting linear models (e.g., factors, polynomial regression) to distance matrices; uses a permutation test with pseudo-F ratios.

dune.ado<-adonis(formula = dune~Management, data = dune.env, method = "bray")
dune.ado

# You can also have more advanced models in adonis
dune.ado2<-adonis(dune~Management*A1, data = dune.env)
dune.ado2
```

After a significant ANOSIM, you may want to know which species are primarily responsible for the observed difference between clusters. SIMPER (Similarity Percentage) will do this for you. The test does not come with significance testing. In the output tables, taxa are sorted in descending order of contribution to group difference. 

#### <span style="color:forestgreen"> Compare the different groups and check which species that contributes mostly to the difference between pairs of clusters.  </span>

```{r, warning=FALSE, message=FALSE}
sim <- simper(dune, dune.env$Management) #try management groupings
summary(sim)
```

Visually compare the result from simper with species positions in a biplot.

```{r}
plot(dune.ca, type = "n", scaling = "species")
with(dune.env, points(dune.ca, display = "sites", scaling = "species", col = as.numeric(Management), pch=16))
with(dune.env, orditorp(dune.ca, display = "species", scaling = "species"))
with(dune.env, ordihull(dune.ca, Management, scaling = "species",label = TRUE))
```

<br>

A complementary/alternative approach to identifying which species are most reposnisble for the differences between the groups is that of indicator species - which species best characterise the groups or clusters that we have identified? 

In this exercise we will investigate if any particular species are indicative for the four different management types. Use the resulting data table to find the two top indicator species for each of the four Management types! 

```{r, warning=FALSE, message=FALSE}
library(labdsv)
library(tibble)
iva <- indval(dune, dune.env.original$Management) #this time we don't need dummy variables for management
iva.df <- as.data.frame(iva$indval)
#arrange in descending order to find best indicator species, change BF to other management types as needed
arrange(rownames_to_column(iva.df), desc(HF))
```

<br>

***


### <span style="color:cornflowerblue">Question 29: Molecular Data</span>

#### <span style="color:forestgreen">Molecular </span>

```{r}
#to follow, Stefan
```
***
<br>

### <span style="color:cornflowerblue">Question 30: PLS </span>

#### <span style="color:forestgreen"> Examine the p-values to explore which response variables actually give a significant result. </span>

PLS regression, like PCA, seeks to find components which maximize the variability of predictors but differs from PCA as PLS requires the components to have maximum correlation with the response. The predictor variables are mapped to a smaller set of variables, and within that smaller space we perform a regression against the outcome variable. PLS aims to choose new mapped variables that maximally explain the outcome variable.

```{r PLS, message=FALSE, warning=FALSE}
data.pls <- readRDS("Trend_Lakes_2015_PLS.RDS")
#View(data.pls)
library(pls)
#This function takes the same form as a lot of regression models in R:
#plsr(Response variable ~ Explanatory Variables, data = yourdata, scale = TRUE/FALSE)

pls.fit <- plsr(Species_number ~ ., data = data.pls[c(9:34)], scale = TRUE,
                na.action = na.omit, validation = "CV")

#The next step is to remove unwanted variables and then build a model.  
#Cross validation is used to find the optimal number of retained dimensions.
#Then the model is rebuilt with this optimal number of dimensions. 
#Find the number of dimensions with lowest cross validation error
cv <- RMSEP(pls.fit)
best.dims <- which.min(cv$val[estimate = "adjCV", , ]) - 1
best.dims 
# Rerun the model
pls.fit2 <-  plsr(Species_number ~ ., data = data.pls[c(6:34)], ncomp = best.dims)
#Finally, we extract the useful information and format the output.
coefficients <-  coef(pls.fit2)
sum.coef <-  sum(sapply(coefficients, abs))
coefficients <-  coefficients * 100 / sum.coef
coefficients <-  sort(coefficients[, 1 , 1])
barplot(tail(coefficients, 3))
#The regression coefficients are normalized so their absolute sum is 100 and the result is #sorted.
#The results below show that 'Total_N' and 'Mn' are the two most important positive #predictors of 'Species_count'.  You could run the following to see the other end of the scale for negative predictors.
barplot(head(coefficients, 3))
#Here we see that "Fe", iron is the most important negative predictor.
summary(pls.fit2)
coefplot(pls.fit2)

```

We can try a different reponse variable, Biovolume, and also look at the scores and loadings. 
```{r}
pls.fit.b <- plsr(Biovolume ~ ., data = data.pls[c(9:34)], scale = TRUE,
                na.action = na.omit, validation = "CV")

#Find the number of dimensions with lowest cross validation error
cv <- RMSEP(pls.fit.b)
best.dims <- which.min(cv$val[estimate = "adjCV", , ]) - 1
best.dims 
# Rerun the model
pls.fit.b2 <-  plsr(Biovolume ~ ., data = data.pls[c(6:34)], ncomp = best.dims)
#Finally, we extract the useful information and format the output.
coefficients <-  coef(pls.fit.b2)
sum.coef <-  sum(sapply(coefficients, abs))
coefficients <-  coefficients * 100 / sum.coef
coefficients <-  sort(coefficients[, 1 , 1])
barplot(tail(coefficients, 3))
barplot(head(coefficients, 3))

#The loading plot is often used for interpretation purposes, for instance to look for known #spectral peaks or profiles:
plot(pls.fit.b2, "loadings", comps = 1:2, legendpos = "topleft", xlab = "nm")
abline(h = 0)

#This gives a pairwise plot of the score values for the three first components.
#Score plots are often used to look for patterns, groups or outliers in the data. 
plot(pls.fit.b2, plottype = "scores", comps = 1:3)
plot(pls.fit.b2, ncomp = 2, asp = 1, line = TRUE)
```
